(this.webpackJsonpprojectize=this.webpackJsonpprojectize||[]).push([[0],{129:function(e,a,t){},187:function(e,a,t){e.exports=t(323)},192:function(e,a,t){},323:function(e,a,t){"use strict";t.r(a);var n=t(0),r=t.n(n),o=t(10),i=t.n(o),l=t(120),c=t(11),m=t(177),s=t(374),u=t(375),g=(t(192),t(129),t(368)),d=t(327),h=t(378),p=t(364),f=t(362),E=t(365),b=t(369),v=t(370),y=t(371),C=t(372),k=t(358),w=t(79),S=t.n(w),x=t(80),I=t.n(x),N="https://github.com/arcchang1236/CA-NoiseGAN",B="Learning Camera-Aware Noise Models",j='Ke-Chi Chang, Ren Wang, Hung-Jin Lin, Yu-Lun Liu, Chia-Ping Chen, Yu-Lin Chang, and Hwann-Tzong Chen "Learning Camera-Aware Noise Models", in Proceedings of the European Conference on Computer Vision (ECCV), 2020',F="",L="",T="",D="",M=[{name:"Ke-Chi Chang",url:"",affiliation:"1"},{name:"Ren Wang",url:"",affiliation:"2"},{name:"Hung-Jin Lin",url:"",affiliation:"2"},{name:"Yu-Lun Liu",url:"",affiliation:"2"},{name:"Chia-Ping Chen",url:"",affiliation:"2"},{name:"Yu-Lin Chang",url:"",affiliation:"2"},{name:"Hwann-Tzong Chen",url:"",affiliation:"1"}],O=[{number:"1",name:"National Tsing Hua University",url:""},{number:"2",name:"MediaTek Inc.",url:""}],R=t(122),A=t(360),H=t(172),W=Object(k.a)((function(e){var a;return{toolbar:{borderBottom:"1px solid ".concat(e.palette.divider),backgroundColor:"#003153",color:"#fff",zIndex:0},toolbarTitle:{flex:1},toolbarSecondary:(a={justifyContent:"center",backgroundColor:"#ddd",zIndex:0,"& > a":{padding:e.spacing(0,5)}},Object(R.a)(a,e.breakpoints.down("md"),{"& > a":{padding:e.spacing(0,3)}}),Object(R.a)(a,e.breakpoints.down("xs"),{"& > a":{padding:e.spacing(0,1)}}),a),toolbarLink:{padding:e.spacing(1),flexShrink:0}}}));function P(e){var a=W(),t=e.sections,n=e.title,o=function(e){return e.preventDefault()};return r.a.createElement(r.a.Fragment,null,r.a.createElement(A.a,{className:a.toolbar},r.a.createElement(d.a,{component:"h2",variant:"h5",color:"inherit",align:"center",noWrap:!0,className:a.toolbarTitle},n)),r.a.createElement(A.a,{component:"nav",variant:"dense",className:a.toolbarSecondary},t.map((function(e){return r.a.createElement(f.a,{component:H.HashLink,onClick:o,variant:"subtitle2",color:"inherit",noWrap:!0,smooth:!0,to:e.url,key:e.title,className:a.toolbarLink},e.title)}))))}var _=t(377),Y=t(363),z=Object(k.a)((function(e){return{title:{padding:e.spacing(3,0),margin:e.spacing(6,0,0,0)},text:{fontFamily:"Source Serif Pro"}}}));function G(e){var a=z(),t=e.name,n=e.anchor,o=e.variant;return r.a.createElement(_.a,{className:a.title,id:n},r.a.createElement(d.a,{className:a.text,align:"left",component:o.component,variant:o.variant,gutterBottom:!0},t),r.a.createElement(Y.a,null))}G.defaultProps={variant:{component:"h2",variant:"h4"}};var V=Object(k.a)((function(e){return{authorRoot:{flexGrow:1},affiliationRoot:{flexGrow:1,padding:e.spacing(0,0,6)}}}));function J(e){var a=V(),t=e.authors,n=e.affiliations;return r.a.createElement(r.a.Fragment,null,r.a.createElement(p.a,{container:!0,justify:"center",className:a.authorRoot,spacing:2},t.map((function(e,a){return r.a.createElement(p.a,{item:!0,key:a},r.a.createElement(d.a,{component:"h3",variant:"h6"},r.a.createElement(f.a,{href:e.url},e.name),r.a.createElement("sup",null,e.affiliation)))}))),r.a.createElement(p.a,{container:!0,justify:"center",className:a.affiliationRoot,spacing:2},n.map((function(e,a){return r.a.createElement(p.a,{item:!0,key:a},r.a.createElement(d.a,{component:"h4",variant:"h6"},e.name,r.a.createElement("sup",null,e.number)))}))))}var U=t(366),K=Object(k.a)((function(e){return{bannerImg:{padding:e.spacing(2,0,6),backgroundColor:"transparent"}}}));function q(e){var a=K(),t=e.metadata,n=e.imageSrc;return r.a.createElement(E.a,{elevation:t.elevation,className:a.bannerImg},r.a.createElement(p.a,{container:!0},r.a.createElement(f.a,{href:t.link,target:"_blank",rel:"noopener"},r.a.createElement(d.a,{component:"h3",variant:"subtitle1",color:"inherit",gutterBottom:!0},t.title),r.a.createElement(U.a,{component:"img",alt:t.title,src:n,title:t.title}))))}t(367),Object(k.a)({root:{background:"linear-gradient(87deg, #FE6B8B 30%, #FF8E63 90%)",border:0,borderRadius:3,boxShadow:"0 3px 5px 2px rgba(255, 105, 135, .3)",color:"white",height:48,padding:"0 30px"}});function X(){return r.a.createElement(d.a,{variant:"body2",color:"textSecondary",align:"center"},"Copyright \xa9 ",r.a.createElement(f.a,{color:"inherit",href:"https://github.com/leVirve"},"Hung-Jin Lin")," ",(new Date).getFullYear(),".")}var Q=Object(k.a)((function(e){return{footer:{padding:e.spacing(6,0)}}}));function $(e){var a=Q(),t=e.description,n=e.title;return r.a.createElement("footer",{className:a.footer},r.a.createElement(g.a,{maxWidth:"lg"},r.a.createElement(d.a,{variant:"h6",align:"center",gutterBottom:!0},n),r.a.createElement(d.a,{variant:"subtitle1",align:"center",color:"textSecondary",component:"p"},t),r.a.createElement(X,null)))}var Z=Object(k.a)((function(e){return{main:{textAlign:"center"},titleHead:{paddingTop:e.spacing(8),paddingBottom:e.spacing(4),fontFamily:"Source Serif Pro","& > *":{margin:e.spacing(.5)}},bibtexSpan:{backgroundColor:e.palette.grey[200],marginTop:e.spacing(2),padding:e.spacing(1,4)}}})),ee=[{title:"Home",url:"#"},{title:"Abstract",url:"#abstract"},{title:"Paper",url:"#paper"},{title:"Download",url:"#download"},{title:"Results",url:"#results"},{title:"Acknowledgments",url:"#acknowledgments"}];var ae=function(){var e=Z();return r.a.createElement("div",{className:e.main},r.a.createElement(P,{id:"header",title:B,sections:ee}),r.a.createElement(I.a,{top:"#header"},r.a.createElement(S.a,{href:N})),r.a.createElement(g.a,{maxWidth:"lg"},r.a.createElement(d.a,{component:"h1",variant:"h3",gutterBottom:!0,className:e.titleHead},B," ",r.a.createElement("br",null),r.a.createElement(h.a,{label:"ECCV 2020"})),r.a.createElement(J,{authors:M,affiliations:O}),r.a.createElement(q,{metadata:{elevation:0},imageSrc:"https://picsum.photos/id/1010/5184/3456"}),r.a.createElement(G,{anchor:"abstract",name:"Abstract"}),r.a.createElement(d.a,{component:"h3",variant:"h6",align:"left",paragraph:!0},"Modeling imaging sensor noise is a fundamental problem for image processing and computer vision applications.\nWhile most previous works adopt statistical noise models, real-world noise is far more complicated and beyond what these models can describe.\nTo tackle this issue, we propose a data-driven approach, where a generative noise model is learned from real-world noise.\nThe proposed noise model is camera-aware, that is, different noise characteristics of different camera sensors can be learned simultaneously,\nand a single learned noise model can generate different noise for different camera sensors.\nExperimental results show that our method quantitatively and qualitatively outperforms existing statistical noise models and learning-based methods.\n"),r.a.createElement(G,{anchor:"paper",name:"Paper"}),r.a.createElement(p.a,{item:!0},r.a.createElement(f.a,{href:F,target:"_blank",rel:"noopener"},r.a.createElement(b.a,{color:"action",style:{fontSize:60}}),r.a.createElement(d.a,{align:"center",variant:"h6",color:"inherit"},"Arxiv"))),r.a.createElement(G,{anchor:"citation",name:"Citation"}),r.a.createElement(d.a,{align:"left",variant:"h6",color:"inherit",gutterBottom:!0},j),r.a.createElement(h.a,{label:"BibTeX",variant:"outlined",color:"primary"}),r.a.createElement(E.a,{elevation:0,className:e.bibtexSpan},r.a.createElement(d.a,{align:"left",variant:"h6",color:"inherit",gutterBottom:!0},r.a.createElement("pre",{style:{wordWrap:"break-word",whiteSpace:"pre-wrap"}},"@inproceedings{\n    TODO: need change\n    Liu-Learning-CVPR-2020,\n    author    = {Liu, Yu-Lun and Lai, Wei-Sheng and Yang, Ming-Hsuan and Chuang, Yung-Yu and Huang, Jia-Bin},\n    title     = {Learning to See Through Obstructions},\n    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},\n    year      = {2020}\n}"))),r.a.createElement(G,{anchor:"download",name:"Download"}),r.a.createElement(p.a,{container:!0,justify:"center",spacing:6},r.a.createElement(p.a,{item:!0},r.a.createElement(f.a,{href:N,target:"_blank",rel:"noopener"},r.a.createElement(v.a,{color:"action",style:{fontSize:60}}),r.a.createElement(d.a,{align:"center",variant:"h6",color:"inherit"},"Code"))),r.a.createElement(p.a,{item:!0},r.a.createElement(f.a,{href:T,target:"_blank",rel:"noopener"},r.a.createElement(b.a,{color:"action",style:{fontSize:60}}),r.a.createElement(d.a,{align:"center",variant:"h6",color:"inherit",gutterBottom:!0},"Supplementary"))),r.a.createElement(p.a,{item:!0},r.a.createElement(f.a,{href:D,target:"_blank",rel:"noopener"},r.a.createElement(y.a,{color:"action",style:{fontSize:60}}),r.a.createElement(d.a,{align:"center",variant:"h6",color:"inherit",gutterBottom:!0},"Results"))),r.a.createElement(p.a,{item:!0},r.a.createElement(f.a,{href:L,target:"_blank",rel:"noopener"},r.a.createElement(C.a,{color:"action",style:{fontSize:60}}),r.a.createElement(d.a,{align:"center",variant:"h6",color:"inherit"},"Video")))),r.a.createElement(G,{anchor:"acknowledgments",name:"Acknowledgments"}),r.a.createElement(d.a,{variant:"body1",align:"left",paragraph:!0},"We thank ",r.a.createElement(f.a,{href:"https://github.com/leVirve"},"Hung-Jin Lin")," for providing this template! ",r.a.createElement("br",null),"We thank ",r.a.createElement(f.a,{href:""},"SIDD")," for providing their datasets. ",r.a.createElement("br",null),"Part of our codes are based on ",r.a.createElement(f.a,{href:""},"SIDD simple camera pipeline")," and ",r.a.createElement(f.a,{href:""},"NoiseFlow"),"."),r.a.createElement($,null)))},te=t(88),ne=t(379),re=t(373),oe=t(380),ie=t(376),le=t(381),ce=t(4),me=t(176),se=t.n(me);for(var ue=function(e){var a=e.src,t=e.title;return r.a.createElement(_.a,null,r.a.createElement(d.a,{variant:"subtitle1",align:"center"},t),r.a.createElement(se.a,{color:"transparent",style:{paddingTop:10},imageStyle:{height:"235px",width:"inherit",left:"auto",position:"relative"},src:a}))},ge=Object(k.a)((function(e){return{slider:{width:300},formControl:{margin:e.spacing(1),minWidth:200},selectText:{"&.Mui-focused":{color:"#FFB6C1"}},select:{"&:before":{borderBottom:"1px solid #F3B3B1"},"&:after":{borderBottom:"3px solid #FFB6C1"},"&:hover:not(.Mui-disabled):not(.Mui-focused):not(.Mui-error):before":{borderBottom:"2px solid #FFB6C1"}}}})),de=Object(ce.a)({root:{color:"#F08080",height:8},thumb:{height:12,width:12,backgroundColor:"#fff",border:"2px solid currentColor"},valueLabel:{left:"calc(-12px)"},mark:{backgroundColor:"#FFB6C1",height:3,borderRadius:2},track:{height:4,borderRadius:2,background:"linear-gradient(87deg, #FE6B8B 30%, #FF8E63 90%)"},rail:{height:4,borderRadius:2,background:"linear-gradient(87deg, #FE6B8B 30%, #FF8E63 90%)"}})(ne.a),he=[],pe=2;pe<25;pe+=2)he.push({value:pe});for(var fe=[],Ee=2;Ee<10;Ee++)fe.push({value:Ee});var be=[{title:"Home",url:"/"},{title:"Control",url:"#control"},{title:"Result",url:"#result"},{title:"References",url:"#references"}];function ve(e,a,t){var n=t.imageId,r=t.styleId,o="".concat("/arcchang1236/CA-NoiseGAN/images/demo_compared","/").concat(e);return void 0===n?"".concat(o,"/").concat(a,"_").concat(ye(r,2),".jpg"):void 0===r?"".concat(o,"/").concat(a,"_").concat(ye(n,2),".jpg"):"".concat(o,"/").concat(a,"_").concat(ye(n,2),"_").concat(ye(r,2),".jpg")}function ye(e,a){var t=Math.max(0,a-e.toString().length);return Math.pow(10,t).toString().substr(1)+e}var Ce=function(){var e=ge(),a=Object(n.useState)(1),t=Object(te.a)(a,2),o=t[0],i=t[1],l=Object(n.useState)(1),c=Object(te.a)(l,2),m=c[0],s=c[1],u=Object(n.useState)("dog2cat"),h=Object(te.a)(u,2),E=h[0],b=h[1];return r.a.createElement("div",{className:"App"},r.a.createElement(P,{id:"header",title:B,sections:be}),r.a.createElement(I.a,{top:"#header"},r.a.createElement(S.a,{href:N})),r.a.createElement(g.a,{maxWidth:"lg"},r.a.createElement(G,{anchor:"control",name:"Control",variant:{componet:"h3",variant:"h5"}}),r.a.createElement(d.a,{variant:"h6",align:"left",paragraph:!0},"Here you can browse the results of our model in comparison to state-of-the-arts by choosing the translation tasks for different datasets, content image ID (from 1 to 25), and style image ID (from 1 to 10)."),r.a.createElement(p.a,{container:!0,justify:"center",spacing:6},r.a.createElement(p.a,{item:!0},r.a.createElement(re.a,{className:e.formControl},r.a.createElement(oe.a,{id:"label",className:e.selectText},"Dataset"),r.a.createElement(ie.a,{className:e.select,value:E,onChange:function(e){b(e.target.value)}},r.a.createElement(le.a,{value:"dog2cat"},"Dog \u2192 Cat"),r.a.createElement(le.a,{value:"cat2dog"},"Cat \u2192 Dog"),r.a.createElement(le.a,{value:"monet"},"Photo \u2192 Monet"),r.a.createElement(le.a,{value:"portrait"},"Photo \u2192 Portrait")))),r.a.createElement(p.a,{item:!0},r.a.createElement(d.a,{variant:"subtitle1",gutterBottom:!0},"Content Image ID:"),r.a.createElement(de,{className:e.slider,value:o,"aria-labelledby":"discrete-slider",valueLabelDisplay:"auto",step:1,marks:he,min:1,max:25,onChange:function(e,a){i(a)}})),r.a.createElement(p.a,{item:!0},r.a.createElement(d.a,{variant:"subtitle1",gutterBottom:!0},"Style Image ID:"),r.a.createElement(de,{className:e.slider,value:m,"aria-labelledby":"discrete-slider",valueLabelDisplay:"auto",step:1,marks:fe,min:1,max:10,onChange:function(e,a){s(a)}})),r.a.createElement(p.a,{container:!0,justify:"center",spacing:6},r.a.createElement(p.a,{item:!0},r.a.createElement(ue,{title:"Content Image",src:ve(E,"content",{imageId:o})})),r.a.createElement(p.a,{item:!0},r.a.createElement(ue,{title:"Style Image",src:ve(E,"style",{styleId:m})})))),r.a.createElement(G,{anchor:"result",name:"Result",variant:{componet:"h3",variant:"h5"}}),r.a.createElement(d.a,{variant:"h6",align:"left",paragraph:!0},"With the given content and style image, here demonstrates the generated result from MUNIT, GDWCT, MSGAN, and Ours."),r.a.createElement(p.a,{container:!0,justify:"center",spacing:1},r.a.createElement(p.a,{item:!0,xs:6,md:3},r.a.createElement(ue,{title:"MUNIT [1]",src:ve(E,"munit",{imageId:o,styleId:m})})),r.a.createElement(p.a,{item:!0,xs:6,md:3},r.a.createElement(ue,{title:"GDWCT [2]",src:ve(E,"gdwct",{imageId:o,styleId:m})})),r.a.createElement(p.a,{item:!0,xs:6,md:3},r.a.createElement(ue,{title:"MSGAN [3]",src:ve(E,"msgan",{imageId:o,styleId:m})})),r.a.createElement(p.a,{item:!0,xs:6,md:3},r.a.createElement(ue,{title:"Ours",src:ve(E,"ours",{imageId:o,styleId:m})}))),r.a.createElement("br",null),r.a.createElement(G,{anchor:"references",name:"References",variant:{componet:"h3",variant:"h5"}}),r.a.createElement(d.a,{align:"left",variant:"body1",color:"inherit",gutterBottom:!0},"[1] ",r.a.createElement(f.a,{href:"https://arxiv.org/abs/1804.04732",target:"_blank",rel:"noopener"},'Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, "Multimodal Unsupervised Image-to-Image Translation"'),", ECCV 2018 ",r.a.createElement("br",null),"[2] ",r.a.createElement(f.a,{href:"https://arxiv.org/abs/1812.09912",target:"_blank",rel:"noopener"},'Wonwoong Cho, Sungha Choi, David Keetae Park, Inkyu Shin, Jaegul Choo, "Image-to-Image Translation via Group-wise Deep Whitening-and-Coloring Transformation"'),", CVPR 2019 ",r.a.createElement("br",null),"[3] ",r.a.createElement(f.a,{href:"https://arxiv.org/abs/1903.05628",target:"_blank",rel:"noopener"},'Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang, "Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis"'),", CVPR 2019"),r.a.createElement($,null)))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));var ke=Object(m.a)({typography:{fontFamily:["Source Sans Pro","-apple-system","BlinkMacSystemFont",'"Segoe UI"',"Roboto",'"Helvetica Neue"',"Arial","sans-serif",'"Apple Color Emoji"','"Segoe UI Emoji"','"Segoe UI Symbol"'].join(",")},palette:{primary:{main:"#666"},secondary:{main:"#F092A0"}}});i.a.render(r.a.createElement(s.a,{theme:ke},r.a.createElement("div",null,r.a.createElement(u.a,null),r.a.createElement(l.HashRouter,{basename:"/arcchang1236/CA-NoiseGAN"},r.a.createElement(c.d,{exact:!0,path:"/",component:ae}),r.a.createElement(c.d,{exact:!0,path:"/demo",component:Ce})))),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()})).catch((function(e){console.error(e.message)}))}},[[187,1,2]]]);
//# sourceMappingURL=main.7f536acc.chunk.js.map